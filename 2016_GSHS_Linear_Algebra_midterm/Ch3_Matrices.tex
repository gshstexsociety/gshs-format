\chapter{Matrices}
\section{Terminology}

\textit{Definition.} A \textbf{matrix} is a rectangular array of numbers, which are called as \textbf{entries} or \textbf{elements}. If the matrix has $n$ rows and $m$ columns, the \textbf{size} of the matrix is $n \times m$.

\noindent \\ A $1 \times n$ matrix is called a \textbf{row matrix}, or \textbf{row vector}. A $n \times 1$ matrix is called a \textbf{column matrix}, or \textbf{column vector}. (\textit{A vector is considered as a matrix}) We can denote matrices using row vectors or column vectors, such as 
\begin{align*}
A = \begin{bmatrix}
\textbf{A}^{C}_1 & \textbf{A}^{C}_2 & \cdots & \textbf{A}^{C}_m
\end{bmatrix} = \begin{bmatrix}
\textbf{A}^{R}_1 \\ \textbf{A}^{R}_2 \\ \vdots \\ \textbf{A}^{R}_n
\end{bmatrix}
\end{align*} where $\textbf{A}^{C}_i$ is the $i$th column of $A$ and $\textbf{A}^{R}_i$ is the $i$th row of $A$.

\noindent \\ The element at $i$th row and $j$th column is denoted by $A_{ij}$. We can also denote matrices using elements, such as $A = [A_{ij}]$.

\noindent \\ \textit{Definition.} The \textbf{diagonal entries}of $A$ are $A_{ii}$.

\noindent \\ \textit{Definition.} The \textbf{square matrix} is a matrix which has same number of rows and columns (so the size is $n \times n$). \textbf{Diagonal matrix} is a square matrix which has its nondiagonal entries as 0. A diagonal matrix with all of its diagonal entries are the same are \textbf{scalar matrix}. If the value of diagonal entries are all 1, it is \textbf{identity matrix}.

\noindent A $n \times n$ identity matrix is denoted as $I_n$, and
\begin{align*}
I_n = \begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}
\end{align*}

\noindent A \textbf{zero matrix} $O$ is a matrix which all of its entires are zero.

\noindent \\ \textit{Definition.} Two matrices are \textbf{equal} if and only if

(i) The size of two matrices are the same.

(ii) The corresponding entries of the matrices are the same.

\section{Matrix Operations}

\textit{Definition.} If $A$ and $B$ are both $n \times m$ matrices, the \textbf{sum} of $A$ and $B$ is defined as
\begin{align*}
A + B = [A_{ij} + B_{ij}]
\end{align*} where $A+B$ is also an $n \times m$ matrix.

\noindent \\ \textit{Definition.} If $A$ is an $n \times m$ matrix and $c$ is a scalar, the \textbf{scalar multiplication} $cA$ is defined as
\begin{align*}
cA = [cA_{ij}]
\end{align*} where $cA$ is also an $n \times m$ matrix.

\noindent \\ \textit{Definition.} If $A$ is an $m \times n$ matrix and $B$ is an $n \times r$ matrix, then the \textbf{product} $AB$ is defined as
\begin{align*}
AB = [A_{i1}B_{1j} + A_{i2}B_{2j} + \cdots + A_{in}B_{nj}]
\end{align*} where $AB$ is an $m \times r$ matrix.

\noindent \\ \textit{* Note.} $(AB)_{ij} = \textbf{A}^{R}_i \cdot \textbf{B}^{C}_j$

\noindent \\ \textit{Defintion.} A \textbf{transpose} of an $n \times m$ matrix $A$ is denoted as $A^{T}$, which is an $m \times n$ matrix and
\begin{align*}
(A^{T})_{ij} = A_{ji}
\end{align*}

\noindent \\ \textit{Definition.} A square matrix $A$ is \textbf{symmetric} if $A^{T} = A$. \\


\noindent \\ Matrices can be divided into \textbf{submatrices} by partitioning the matrix into ceratin blocks. We introduce partitioned matrix in order to perform matrix multiplication in easier way.

\begin{plaintheorem}[Multiplication of Partitioned Matrices]
	If matrices $A$ and $B$ are partitioned as $A = \begin{bmatrix}
	A_{11} & \cdots & A_{1m} \\
	\vdots &        & \vdots \\
	A_{n1} & \cdots & A_{nm}
	\end{bmatrix}$ and $B = \begin{bmatrix}
	B_{11} & \cdots & B_{1r} \\
	\vdots &        & \vdots \\
	B_{m1} & \cdots & B_{mr}
	\end{bmatrix}$, then $AB$ can be partitioned as
	\begin{align*}
	AB = \begin{bmatrix}
	A_{11}B_{11} + A_{12}B_{21} + \cdots + A_{1m}B_{m1}
	& \cdots & A_{11}B_{1r} + A_{12}B_{2r} + \cdots + A_{1m}B_{mr} \\
	\vdots & & \vdots \\
	A_{n1}B_{11} + A_{n2}B_{21} + \cdots + A_{nm}B_{m1}
	& \cdots & A_{n1}B_{1r} + A_{n2}B_{2r} + \cdots + A_{nm}B_{mr}
	\end{bmatrix}
	\end{align*} assuming that all the products are defined.
\end{plaintheorem}
\begin{proof}
	Let $A$ be an $n \times m$ matrix and let $B$ be an $m \times r$ matrix so that $AB$ is defined. Then
	\begin{align*}
	AB = A\begin{bmatrix}
	\textbf{B}^{C}_1 & \textbf{B}^{C}_2 & \cdots & \textbf{B}^{C}_r
	\end{bmatrix} = \begin{bmatrix}
	A\textbf{B}^{C}_1 & A\textbf{B}^{C}_2 & \cdots & A\textbf{B}^{C}_r
	\end{bmatrix} &\textnormal{ (matrix-column representation)} \\
	= \begin{bmatrix}
	\textbf{A}^{R}_1 \\ \textbf{A}^{R}_2 \\ \vdots \\ \textbf{A}^{R}_n
	\end{bmatrix}B = \begin{bmatrix}
	\textbf{A}^{R}_1B \\ \textbf{A}^{R}_2B \\ \vdots \\ \textbf{A}^{R}_nB
	\end{bmatrix} &\textnormal{ (row-matrix representation)} \\
	= \begin{bmatrix}
	\textbf{A}^{R}_1 \\ \textbf{A}^{R}_2 \\ \vdots \\ \textbf{A}^{R}_n
	\end{bmatrix}\begin{bmatrix}
	\textbf{B}^{C}_1 & \textbf{B}^{C}_2 & \cdots & \textbf{B}^{C}_r
	\end{bmatrix} = \begin{bmatrix}
	\textbf{A}^{R}_1\textbf{B}^{C}_1 & \cdots &
	\textbf{A}^{R}_1\textbf{B}^{C}_r \\
	\vdots & & \vdots \\
	\textbf{A}^{R}_n\textbf{B}^{C}_1 & \cdots &
	\textbf{A}^{R}_n\textbf{B}^{C}_r		
	\end{bmatrix} &\textnormal{ (row-column representation)} \\
	= \begin{bmatrix}
	\textbf{A}^{C}_1 & \textbf{A}^{C}_2 & \cdots & \textbf{A}^{C}_m
	\end{bmatrix} \begin{bmatrix}
	\textbf{B}^{R}_1 \\ \textbf{B}^{R}_2 \\ \vdots \\ \textbf{B}^{R}_m
	\end{bmatrix} &\textnormal{ (column-row representation)}\\
	= \textbf{A}^{C}_1\textbf{B}^{R}_1 + \cdots + \textbf{A}^{C}_m\textbf{B}^{R}_m &\textnormal{ (outer product expansion)}
	\end{align*}
\end{proof} 

\begin{theorem}
	Let $A$ be an $n \times m$ matrix. Then
	\begin{enumerate}
		\item $\textbf{e}_iA = \textbf{A}^{R}_i$
		\item $A\textbf{e}_j = \textbf{A}^{C}_j$
	\end{enumerate}
\end{theorem}

\begin{proof}
	\textbf{(a)} (Exercise 3.1 41) Since the size of $\textbf{e}_i$ is $1 \times n$ and the size of $A$ is $n \times m$, the size of $\textbf{e}_iA$ is $1 \times m$, which is same with the size of $\textbf{A}^{R}_i$. Also,
	\begin{align*}
	(\textbf{e}_iA)_{1k} = 0A_{1k} + \cdots + 1A_{ik} + \cdots + 0A_{nk} = A_{ik}
	\end{align*} Therefore $\textbf{e}_iA = \begin{bmatrix}
	A_{i1} & A_{i2} & \cdots & A_{im}
	\end{bmatrix} = \textbf{A}^{R}_i$.
	
	\noindent \\ \textbf{(b)} Since the size of $A$ is $n \times m$ and the size of $\textbf{e}_j$ is $m \times 1$, the size of $A\textbf{e}_j$ is $n \times 1$, which is same with the size of $\textbf{A}^{C}_i$. Also,
	\begin{align*}
	(A\textbf{e}_j)_{k1} = 0A_{k1} + \cdots + 1A_{ki} + \cdots + 0A_{km} = A_{ki}
	\end{align*} Therefore $A\textbf{e}_j = \begin{bmatrix}
	A_{1i} \\ A_{2i} \\ \vdots \\ A_{ni}
	\end{bmatrix} = \textbf{A}^{C}_i$.
\end{proof}

\begin{theorem}[Algebraic Properties of Matrix Addition and Scalar Multiplication]
	Let $A$, $B$, and $C$ be matrices of the same size and let $c$ and $d$ be scalars.
	\begin{enumerate}
		\item $A+B = B+A$ (Commutativity of Matrix Addition)
		\item $(A+B)+C = A+(B+C)$ (Associativity of Matrix Addition)
		\item $A+O = A$
		\item $A+(-A) = O$
		\item $c(A+B) = cA+cB$ (Left Distributivity of Scalar Multiplication over Matrix Addition)
		\item $(c+d)A = cA+dA$ (Right Distributivity of Scalar Multiplication over Matrix Addition)
		\item $c(dA) = (cd)A$
		\item $1A = A$
	\end{enumerate}	
\end{theorem}

\begin{proof}
	The size of all matrices in both sides of equation will be equal to the size of $A$, $B$, and $C$. The proof in componentwise perspective is on below.
	\begin{enumerate}
		\item (Exercise 3.2 17)
		\begin{align*}
		(A+B)_{ij} = A_{ij} + B_{ij} = B_{ij} + A_{ij} = (B+A)_{ij}
		\end{align*}
		\item (Exercise 3.2 17)
		\begin{align*}
		((A+B)+C)_{ij} = (A+B)_{ij} + C_{ij} &= (A_{ij} + B_{ij}) + C_{ij} \\
		&= A_{ij} + (B_{ij} + C_{ij}) = A_{ij} + (B+C)_{ij} = (A+(B+C))_{ij}
		\end{align*}
		\item (Exercise 3.2 17)
		\begin{align*}
		(A+O)_{ij} = A_{ij} + O_{ij} = A_{ij} + 0 = A_{ij}
		\end{align*}
		\item (Exercise 3.2 17)
		\begin{align*}
		(A+(-A))_{ij} = A_{ij} + (-A)_{ij} = A_{ij} + (-A_{ij}) = 0
		\end{align*}
		\item (Exercise 3.2 18)
		\begin{align*}
		(c(A+B))_{ij} = c(A+B)_{ij} = c(A_{ij} + B_{ij}) = cA_{ij} + cB_{ij} = (cA)_{ij} + (cB)_{ij} = (cA + cB)_{ij}
		\end{align*}
		\item (Exercise 3.2 18)
		\begin{align*}
		((c+d)A)_{ij} = (c+d)A_{ij} = cA_{ij} + dA_{ij} = (cA)_{ij} + (dA)_{ij} = (cA + dA)_{ij}
		\end{align*}
		\item (Exercise 3.2 18)
		\begin{align*}
		(c(dA))_{ij} = c(dA)_{ij} = c(dA_{ij}) = (cd)A_{ij} = ((cd)A)_{ij}
		\end{align*}
		\item (Exercise 3.2 18)
		\begin{align*}
		(1A)_{ij} = 1A_{ij} = A_{ij}
		\end{align*}
	\end{enumerate}
\end{proof}

\begin{theorem} [Properties of Matrix Multiplication]
	Let $A$, $B$, and $C$ be matrices and let $k$ be the scalar. If the operations can be defined,
	\begin{enumerate}
		\item $A(BC) = (AB)C$
		\item $A(B+C) = AB + AC$
		\item $(A+B)C = AC + BC$
		\item $k(AB) = (kA)B = A(kB)$
		\item $I_nA = A = AI_m$, where size of $A$ is $n \times m$
	\end{enumerate}
\end{theorem}

\begin{proof} The proof of (a) is also done in page 229 of the textbook. (Not Included in Exam)
	\begin{enumerate}
		\item Let $A$ be a $n \times m$ matrix, $B$ be a $m \times p$ matrix, and $C$ be a $p \times q$ matrix. Then the size of $BC$ is $m \times q$, so the size of $A(BC)$ is $n \times q$. Also, the size of $AB$ is $n \times p$, so the size of $(AB)C$ is $n \times q$. Therefore, the size of $A(BC)$ and $(AB)C$ are the same.
		\begin{align*}
		((AB)C)_{ij} &= \sum_{k=1}^{p}((AB)_{ik}C_{kj}) \\
		&= \sum_{k=1}^{p}(\sum_{l=1}^{m}A_{il}B_{lk})C_{kj} \\
		&= \sum_{k=1}^{p}\sum_{l=1}^{m}(A_{il}B_{lk}C_{kj}) \\
		&= \sum_{l=1}^{m}A_{il}(\sum_{k=1}^{p}B_{lk}C_{kj}) \\
		&= \sum_{l=1}^{m}A_{il}(BC)_{lj} \\
		&= (A(BC))_{ij}
		\end{align*}
		
		\item Let $A$ be a $n \times m$ matrix, and let $B$ and $C$ be $m \times r$ matrices. Then the size of $B+C$ is $m \times r$, so the size of $A(B+C)$ is $n \times r$. The size of $AB$ and $AC$ are also $n \times r$, so the size of matrices at both sides of equations are the same.
		\begin{align*}
		(A(B+C))_{ij} &= \textbf{A}^{R}_i \cdot ((B+C)^{C}_j) \\
		&= \textbf{A}^{R}_i \cdot (\textbf{B}^{C}_j + \textbf{C}^{C}_j) \\
		&= \textbf{A}^{R}_i \cdot \textbf{B}^{C}_j + \textbf{A}^{R}_i \cdot \textbf{C}^{C}_j \\
		&= (AB)_{ij} + (AC)_{ij} = (AB+AC)_{ij}
		\end{align*}
		
		\item (Exercise 3.2 19) Let $A$ and $B$ be $n \times m$ matrices, and let $C$ be a $m \times r$ matrix. Then the size of $A+B$ is $n \times m$, so the size of $(A+B)C$ is $n \times r$. The size of $AC$ and $BC$ are also $n \times r$, so the size of matrices at both sides of equations are the same.
		\begin{align*}
		((A+B)C)_{ij} &= ((A+B)^{R}_i) \cdot \textbf{C}^{C}_j \\
		&= (\textbf{A}^{R}_i + \textbf{B}^{R}_i) \cdot \textbf{C}^{C})_j \\
		&= \textbf{A}^{R}_i \cdot \textbf{C}^{C}_j + \textbf{B}^{R}_i \cdot \textbf{C}^{C}_j \\
		&= (AC)_{ij} + (BC)_{ij} = (AC+BC)_{ij} 
		\end{align*}
		
		\item (Exercise 3.2 20) Let $A$ be a $n \times m$ matrix and $B$ be a $m \times r$ matrix. Then the size of $k(AB)$, $(kA)B$, and $A(kB)$ are all $n \times r$, so all the size of the matrices are the same.
		\begin{align*}
		(k(AB))_{ij} &= k(\textbf{A}^{R}_i \cdot \textbf{B}^{C}_j) \\
		&= (k\textbf{A}^{R}_i) \cdot \textbf{B}^{C}_j = ((kA)B)_{ij} \\
		&= \textbf{A}^{R}_i \cdot (k\textbf{B}^{C}_j) = (A(kB))_{ij}
		\end{align*}
		
		\item (Exercise 3.2 21) Let $A$ be a $n \times m$ matrix. Since $I_n = \begin{bmatrix}
		\textbf{e}_1 \\ \vdots \\ \textbf{e}_n
		\end{bmatrix}$ and $I_m = \begin{bmatrix}
		\textbf{e}_1 & \cdots & \textbf{e}_m
		\end{bmatrix}$,
		\begin{align*}
		I_nA = \begin{bmatrix}
		\textbf{e}_1 \\ \vdots \\ \textbf{e}_n
		\end{bmatrix}A = \begin{bmatrix}
		\textbf{e}_1A \\ \vdots \\ \textbf{e}_nA
		\end{bmatrix}& = \begin{bmatrix}
		\textbf{A}^{R}_1 \\ \vdots \\ \textbf{A}^{R}_n
		\end{bmatrix} = A \\
		AI_m = A\begin{bmatrix}
		\textbf{e}_1 & \cdots & \textbf{e}_m
		\end{bmatrix} = \begin{bmatrix}
		A\textbf{e}_1 & \cdots & A\textbf{e}_m
		\end{bmatrix}& = \begin{bmatrix}
		\textbf{A}^{C}_1 & \cdots & \textbf{A}^{C}_m
		\end{bmatrix} = A
		\end{align*}
	\end{enumerate}	
\end{proof}

\noindent \textit{Definition.} For a square matrix $A$ and nonnegative integer $n$, define $A^n$ as
\begin{align*}
A^n = \begin{cases}
I &\mbox{ if } n = 0 \\
A^{n-1}A = AA^{n-1} &\mbox{ otherwise }
\end{cases}
\end{align*} 

\begin{plaintheorem}[Matrix Powers]
	If $A$ is a square matrix and $r, s$ are nonnegative integers,
	\begin{enumerate}
		\item $A^rA^s = A^{r+s}$
		\item $(A^r)^s = A^{rs}$
	\end{enumerate}
\end{plaintheorem}


\begin{proof}
	Let $A$ be an $n \times n$ matrix.
	
	\begin{enumerate}
		\item Claim 1 : For a nonnegative integer $r$, $A^rA^s = A^{r+s}$ for all nonnegative integer $s$.
		
		\noindent (i) $A^rA^0 = A^rI_n = A^r$
		
		\noindent (ii) Suppose that $A^rA^k = A^{r+k}$. Then $A^rA^{k+1} = A^rA^kA = A^{r+k}A = A^{r+k+1}$.
		
		\noindent By (i), (ii), for all nonnegative integer $s$, $A^rA^s = A^{r+s}$.
		
		\noindent \\ Claim 2 : For a nonnegative integer $s$, $A^rA^s = A^{r+s}$ for all nonnegative integer $r$.
		
		\noindent (i) $A^0A^s = I_nA^s = A^s$
		
		\noindent (ii) Suppose that $A^kA^s = A^{k+s}$. Then $A^{k+1}A^s = AA^kA^s = AA^{k+s} = A^{k+s+1}$.
		
		\noindent By (i), (ii), for all nonnegative integer $r$, $A^rA^s = A^{r+s}$.
		
		\noindent \\ By Claim 1 and 2, for all nonnegative integers $r, s$, $A^rA^s = A^{r+s}$.
		
		\item For any nonnegative integer $r$,
		
		\noindent (i) $(A^r)^0 = I_n = A^{0} = A^{r\times0}$
		
		\noindent (ii) Suppose that $(A^r)^k = A^{rk}$. Then $(A^r)^{k+1} = (A^r)^kA^r = A^{rk}A^r = A^{rk+r} = A^{r(k+1)}$.
		
		\noindent By (i), (ii), for all nonnegative integers $r, s$, $(A^r)^s = A^{rs}$.
	\end{enumerate}
\end{proof}

\begin{theorem} [Properties of Transpose]
	Let $A$ and $B$ be matrices and let $k$ be a scalar. If the operations are defined,
	\begin{enumerate}
		\item $(A^{T})^{T} = A$
		\item $(A+B)^{T} = A^{T} + B^{T}$
		\item $(kA)^{T} = k(A^{T})$
		\item $(AB)^{T} = B^{T}A^{T}$
		\item $(A^{r})^{T} = (A^{T})^{r}$ for all r $\in \mathbb{Z}^{+}$
	\end{enumerate}
\end{theorem}

\begin{proof}
	Let $A$ and $B$ be matrices that all the operations are defined.
	\begin{enumerate}
		\item (Exercise 3.2 30) If $A$ is an $n \times m$ matrix, $A^{T}$ is an $m \times n$ matrix, thus $(A^{T})^{T}$ is an $n \times m$ matrix. The size of $(A^{T})^{T}$ and $A$ is the same.
		\begin{align*}
		((A^{T})^{T})_{ij} = (A^{T})_{ji} = A_{ij}			
		\end{align*}
		\item (Exercise 3.2 30) If $A$ and $B$ are both $n \times m$ matrices, the size of $(A+B)$ is also $n \times m$, so $(A+B)^{T}$ and $A^{T}$ and $B^{T}$ are all $m \times n$ matrices.
		\begin{align*}
		((A+B)^{T})_{ij} = (A+B)_{ji} = A_{ji} + B_{ji} = (A^{T})_{ij} + (B^{T})_{ij} = (A^{T} + B^{T})_{ij}
		\end{align*}
		\item (Exercise 3.2 30) If $A$ is an $n \times m$ matrix, both $(kA)^{T}$ and $k(A^{T})$ are $m \times n$ matrices.
		\begin{align*}
		((kA)^{T})_{ij} = (kA)_{ji} = k(A_{ji}) = k(A^{T})_{ij} = (k(A^{T}))_{ij}
		\end{align*}
		\item If $A$ is an $n \times m$ matrix and $B$ is an $m \times r$ matrices, then $(AB)^{T}$ is a $r \times n$ matrix. The size of $B^{T}$ and $A^{T}$ is $r \times m$ and $m \times n$, so the size of $B^{T}A^{T}$ is $r \times n$.
		\begin{align*}
		((AB)^{T})_{ij} = (AB)_{ji} = \textbf{A}^{R}_j \cdot \textbf{B}^{C}_i = (\textbf{A}^{T})^{C}_j \cdot (\textbf{B}^{T})^{R}_i = (B^{T}A^{T})_{ij}
		\end{align*}
		\item (Exercise 3.2 31) Let $A$ be a $n \times n$ matrix.
		
		\noindent (i) $(A^0)^{T} = (I_n)^{T} = I_n = (A^{T})^{0}$
		
		\noindent (ii) Suppose that for an arbitrary nonnegative integer $r$, $(A^r)^T = (A^T)^r$. Then
		\begin{align*}
		(A^{r+1})^T = (A^rA)^T = A^T(A^r)^T = A^T(A^T)^r = (A^T)^{r+1}
		\end{align*}
		By (i) and (ii), $(A^r)^T = (A^T)^r$ for all nonnegative integer $r$.
	\end{enumerate}
\end{proof}

\begin{theorem}
	For any matrix $A$,
	\begin{enumerate}
		\item If $A$ is a square matrix, then $A + A^{T}$ is symmetrical.
		\item $AA^{T}$ and $A^{T}A$ are symmetrical.
	\end{enumerate}
\end{theorem}

\begin{proof}
	\noindent
	\begin{enumerate}
		\item Let $A$ be a square matrix. Then
		\begin{align*}
		(A+A^T)^T = A^T + (A^T)^T = A^T + A = A+A^T
		\end{align*}
		Therefore $A+A^T$ is symmetrical.
		\item (Exercise 3.2 34) Let $A$ be a $n \times m$ matrix. Then the size of $A^T$ is $m \times n$ matrix, so the products $AA^T$ and $A^TA$ are defined.
		\begin{align*}
		& (AA^T)^T = (A^T)^TA^T = AA^T \\
		& (A^TA)^T = A^T(A^T)^T = A^TA
		\end{align*}
		Therefore $AA^T$ and $A^TA$ are symmetrical.
	\end{enumerate}
\end{proof}

\section{Other Types of Matrices : Exercise 3.2}
\textit{Definition.} An \textbf{upper triangular} matrix is a square matrix with all of its entries below the main diagonal are zero. In other words, $A$ is an upper triangular matrix if
\begin{align*}
A_{ij} = 0 \textnormal{ if } i > j
\end{align*}

\begin{plaintheorem}[Properties of Upper Triangular Matrices]
	If $A$ and $B$ are both upper triangular matrices in the same size, the product $AB$ is also an upper triangular matrix.
\end{plaintheorem}

\begin{proof}
	(Exercise 3.2 29)
	
	\noindent (i) If $A$ and $B$ are both $1 \times 1$ upper triangular matrices, let $A = [A_{11}]$ and $B = [B_{11}]$. Then $AB = [A_{11}B_{11}]$, which is also an upper triangular matrix.
	
	\noindent (ii) Suppose that the product of $n \times n$ upper triangular matrices is also upper triangular matrix. Let $A$ and $B$ be both $n+1 \times n+1$ upper triangular matrices. Then we can partition $A$ and $B$ as
	\begin{align*}
	A = \begin{bmatrix}
	A_{11} & A_{12} \\ O & A_{22}
	\end{bmatrix}, B = \begin{bmatrix}
	B_{11} & B_{12} \\ O & B_{22}
	\end{bmatrix}
	\end{align*} where $A_{11}$ and $B_{11}$ are $n \times n$ matrices, $A_{12}$ and $B_{12}$ are $1 \times n$ matrices, and $A_{22}$ and $B_{22}$ are scalars. Since $A$ and $B$ are upper triangular, $A_{11}$ and $B_{11}$ are also upper triangular matrices.
	\begin{align*}
	AB = \begin{bmatrix}
	A_{11}B_{11} + A_{12}O & A_{11}B_{12} + A_{12}B_{22} \\
	OB_{11} + A_{22}O & OB_{12} + A_{22}B_{22}
	\end{bmatrix} = \begin{bmatrix}
	A_{11}B_{11} & A_{11}B_{12} + A_{12}B_{22} \\
	O & A_{22}B_{22}
	\end{bmatrix}
	\end{align*}
	Since $A_{11}B_{11}$ is an upper triangular matrix, $AB$ is also upper triangular.
	
	\noindent By (i) and (ii), for all upper triangular matrices $A$ and $B$, the product $AB$ is also upper triangular. 
\end{proof}

\begin{plaintheorem}[Properties of Symmetric Matrices]
	\begin{enumerate}
		\item If $A$ and $B$ are symmetric, then $A+B$ is also symmetric.
		\item If $A$ is symmetric, then $kA$ is also symmetric for any scalar $k$.
		\item If $A$ and $B$ are symmetric, then $AB$ is symmetric if and only if $AB = BA$.
	\end{enumerate}
\end{plaintheorem}


\begin{proof}
	Let $A$ and $B$ be symmetric matrices that the operations are defined, and let $k$ be a scalar.
	\begin{enumerate}
		\item (Exercise 3.2 35)
		\begin{align*}
		(A+B)^{T} = A^{T} + B^{T} = A+B
		\end{align*} Therefore $A+B$ is symmetric.
		\item (Exercise 3.2 35)
		\begin{align*}
		(kA)^{T} = k(A^{T}) = kA
		\end{align*} Therefore $kA$ is symmetric.
		\item (Exercise 3.2 36)
		\begin{align*}
		(AB)^{T} = B^{T}A^{T} = BA
		\end{align*} Therefore $AB$ is symmetric if and only if $AB = BA$.
	\end{enumerate}
\end{proof}
\noindent \textit{Definition.} A square matrix is \textbf{skew-symmetric} if $A^T = -A$. In other words, $A_{ij} = -A_{ji}$.
\begin{plaintheorem}[Properties of Skew-Symmetric Matrices]
	\begin{enumerate}
		\item If $A$ and $B$ are skew-symmetric, $A+B$ is also skew-symmetric.
		\item If $A$ is a square matrix, then $A-A^T$ is skew-symmetric.
		\item Any square matrix $A$ can be represented as the sum of a symmetric matrix and a skew-symmetric matrix.
	\end{enumerate}
\end{plaintheorem}


\begin{proof}
	Let $A$ and $B$ be square matrices that the operations are defined.
	\begin{enumerate}
		\item (Exercise 3.2 40) Suppose that $A$ and $B$ are skew-symmetric, so that $A^T = -A$ and $B^T = -B$. Then
		\begin{align*}
		(A+B)^T = A^T + B^T = (-A) + (-B) = -(A+B)
		\end{align*} Therefore $A+B$ is also skew-symmetric.
		\item (Exercise 3.2 42)
		\begin{align*}
		(A - A^T)^T = A^T - (A^T)^T = A^T - A = -(A - A^T)
		\end{align*} Therefore $A - A^T$ is also skew-symmetric.
		\item (Exercise 3.2 43) By Theorem : Properties of Symmetric Matrices and Theorem : Properties of Skew-Symmetric Matrices, $A + A^T$ is symmetric and $A - A^T$ is skew-symmetric. Then representing $A$ as
		\begin{align*}
		A = \frac{1}{2}(A + A^T) + \frac{1}{2}(A - A^T)
		\end{align*} which is a sum of symmetric matrix and skew-symmetric matrix.
	\end{enumerate}
\end{proof}

\noindent \textit{Definition.} A \textbf{trace} of a $n \times n$ matrix $A$ is denoted as tr($A$) and defined as
\begin{align*}
\textnormal{tr}(A) = A_{11} + A_{22} + \cdots + A_{nn}
\end{align*}

\begin{plaintheorem}[Properties of the Trace of Matrices]
	Let $A$ and $B$ be $n \times n$ matrices, and let $k$ be a scalar.
	\begin{enumerate}
		\item tr($A+B$) = tr($A$) + tr($B$)
		\item tr($kA$) = $k$tr($A$)
		\item tr($AB$) = tr($BA$)
		\item tr($AA^T$) = $\sum_{1 \le i, j \le n}^{} A_{ij}^{2}$
	\end{enumerate}
\end{plaintheorem}


\begin{proof}
	Let $A$ and $B$ be $n \times n$ matrices, and let $k$ be a scalar.
	\begin{enumerate}
		\item (Exercise 3.2 44)
		\begin{align*}
		\textnormal{tr}(A+B) &= (A+B)_{11} + (A+B)_{22} + \cdots + (A+B)_{nn} \\
		&= (A_{11} + B_{11}) + (A_{22} + B_{22}) + \cdots + (A_{nn} + B_{nn}) \\
		&= (A_{11} + A_{22} + \cdots + A_{nn}) + (B_{11} + B_{22} + \cdots + B_{nn}) = \textnormal{tr}(A) + \textnormal{tr}(B)
		\end{align*}
		\item (Exercise 3.2 44)
		\begin{align*}
		\textnormal{tr}(kA) &= (kA)_{11} + (kA)_{22} + \cdots + (kA)_{nn} \\
		&= k(A_{11} + A_{22} + \cdots + A_{nn}) = k\textnormal{tr}(A)
		\end{align*}
		\item (Exercise 3.2 45)
		\begin{align*}
		\textnormal{tr}(AB) &= (AB)_{11} + (AB)_{22} + \cdots + (AB)_{nn} \\
		&= \sum_{1 \le i, j \le n}^{} A_{ij}B_{ji} \\
		&= \sum_{1 \le i, j \le n}^{} B_{ij}A_{ji} \\
		&= (BA)_{11} + (BA)_{22} + \cdots + (BA)_{nn} = \textnormal{tr}(BA)
		\end{align*}
		\item (Exercise 3.2 46)
		\begin{align*}
		\textnormal{tr}(AA^T) &= (AA^T)_{11} + (AA^T)_{22} + \cdots + (AA^T)_{nn} \\
		&= \sum_{1 \le i, j \le n}^{} A_{ij}(A^T)_{ji} \\
		&= \sum_{1 \le i, j \le n}^{} A_{ij}^{2}
		\end{align*}
	\end{enumerate}
\end{proof}

\section{The Inverse of a Matrix}
\textit{Definition.} If $A$ is an $n \times n$ matrix, an \textbf{inverse} of $A$ is an $n \times n$ matrix, denoted as $\inv{A}$, such that
\begin{align*}
A\inv{A} = I_n \textnormal{ and } \inv{A}A = I_n
\end{align*} If such $\inv{A}$ exists, $A$ is \textbf{invertible}.

\begin{theorem}
	If $A$ is invertible, then its inverse is unique.
\end{theorem}

\begin{proof}
	Suppose that $A'$ and $A''$ are both inveses of $A$. Then
	\begin{align*}
	AA' = I = A'A \textnormal{ and } AA'' = I = A''A
	\end{align*}
	Thus, \begin{align*}
	A' = A'I = A'(AA'') = (A'A)A'' = IA'' = A''
	\end{align*} Therefore, the inverse of $A$ is unique.
\end{proof}

\begin{theorem}
	If $A$ is an invertible $n \times n$ matrix, then the system of linear equations given by $A\textbf{x} =\textbf{b}$ has the unique solution $\textbf{x} = \inv{A}\textbf{b}$.
\end{theorem}

\begin{proof}
	(i) \begin{align*}
	A(\inv{A}\textbf{b}) = (A\inv{A})\textbf{b} = I_n\textbf{b} = \textbf{b}
	\end{align*} Therefore the system is consistent.
	
	\noindent (ii) Suppose that $\textbf{x}'$ is another solution of $A\textbf{x} = \textbf{b}$. Then
	\begin{align*}
	A\textbf{x}' = \textbf{b} & \Rightarrow \inv{A}(A\textbf{x}') = \inv{A}\textbf{b}
	& \Rightarrow (\inv{A}A)\textbf{x}' = \textbf{x}' = \inv{A}\textbf{b}
	\end{align*} Thus, $\textbf{x}'$ is the same solution as before. Therefore, the solution is unique.
\end{proof}

\begin{theorem}
	If $A = \begin{bmatrix}
	a & b \\ c & d
	\end{bmatrix}$, then $A$ is invertible if and only if $ad - bc \neq 0$ (a \textbf{determinant} of $A$, denoted as det $A$), and in that case \begin{align*}
	\inv{A} = \frac{1}{ad-bc}\begin{bmatrix}
	d & -b \\ -c & a
	\end{bmatrix}
	\end{align*}
\end{theorem}

\begin{proof}
	($\Leftarrow$) If $ad - bc \neq 0$,
	\begin{align*}
	\begin{bmatrix}
	a & b \\ c & d
	\end{bmatrix} (\frac{1}{ad-bc}\begin{bmatrix}
	d & -b \\ -c & a
	\end{bmatrix}) &= \frac{1}{ad-bc}\begin{bmatrix}
	ad-bc & 0 \\ 0 & ad-bc
	\end{bmatrix} = \begin{bmatrix}
	1 & 0 \\ 0 & 1
	\end{bmatrix} \\
	(\frac{1}{ad-bc}\begin{bmatrix}
	d & -b \\ -c & a
	\end{bmatrix}) \begin{bmatrix}
	a & b \\ c & d
	\end{bmatrix} &= \frac{1}{ad-bc}\begin{bmatrix}
	ad-bc & 0 \\ 0 & ad-bc
	\end{bmatrix} = \begin{bmatrix}
	1 & 0 \\ 0 & 1
	\end{bmatrix}
	\end{align*}
	Therefore, $A$ is invertible and $\inv{A} = \frac{1}{ad-bc}\begin{bmatrix}
	d & -b \\ -c & a
	\end{bmatrix}$
	
	\noindent ($\Rightarrow$) Suppose that $ad - bc = 0$ and scalars $x, y, z, w$ exist such that \begin{align*}
	\begin{bmatrix}
	a & b \\ c & d
	\end{bmatrix} \begin{bmatrix}
	x & y \\ z & w
	\end{bmatrix} = \begin{bmatrix}
	x & y \\ z & w
	\end{bmatrix} \begin{bmatrix}
	a & b \\ c & d
	\end{bmatrix} = \begin{bmatrix}
	1 & 0 \\ 0 & 1
	\end{bmatrix}
	\end{align*}
	Then \begin{align*}
	\begin{bmatrix}
	a & b \\ c & d
	\end{bmatrix} \begin{bmatrix}
	x & y \\ z & w
	\end{bmatrix} = \begin{bmatrix}
	ax+bz & ay+bw \\ cx+dz & cy+dw
	\end{bmatrix} = \begin{bmatrix}
	1 & 0 \\ 0 & 1
	\end{bmatrix}
	\end{align*}
	Since $ad = bc$, \begin{align*}
	1 = (ax+bz)(cy+dw) &= acxy + adxw + bcyz + bdwz \\
	&= acxy + bcxw + adyz + bdwz = (ay+bw)(cx+dz) = 0
	\end{align*} Thus, such $x, y, z, w$ does not exist, and therefore $A$ is not invertible.
\end{proof}

\begin{theorem} [Properties of Invertible Matrices]
	If $A$, $B$ are invertible matrices of the same size and $c$ is a nonzero scalar,
	\begin{enumerate}
		\item $\inv{A}$ is invertible and \begin{align*}
		\inv{(\inv{A})} = A
		\end{align*}
		\item $cA$ is invertible and \begin{align*}
		\inv{(cA)} = \frac{1}{c}\inv{A}
		\end{align*}
		\item $AB$ is invertible and \begin{align*}
		\inv{(AB)} = \inv{B}\inv{A}
		\end{align*}
		\item $A^T$ is invertible and \begin{align*}
		\inv{(A^T)} = (\inv{A})^{T}
		\end{align*}
		\item $A^n$ is invertible for all nonnegative integers $n$ and \begin{align*}
		\inv{(A^n)} = (\inv{A})^n
		\end{align*}
	\end{enumerate}
\end{theorem}

\begin{proof}
	Let $A$ and $B$ be invertible matrices of the same size and let $c$ be a nonzero scalar.
	\begin{enumerate}
		\item
		\begin{align*}
		\inv{A}A = A\inv{A} = I
		\end{align*} thus $A$ is an inverse of $\inv{A}$.
		\item (Exercise 3.3 14)
		\begin{align*}
		(cA)(\frac{1}{c}\inv{A}) &= (c\frac{1}{c})(A\inv{A}) = I \\
		(\frac{1}{c}\inv{A})(cA) &= (\frac{1}{c}c)(\inv{A}A) = I
		\end{align*} thus $\frac{1}{c}\inv{A}$ is an inverse of $cA$.
		\item
		\begin{align*}
		(AB)(\inv{B}\inv{A}) &= A(B\inv{B})\inv{A} = A\inv{A} = I \\
		(\inv{B}\inv{A})(AB) &= \inv{B}(\inv{A}A)B = \inv{B}B = I
		\end{align*} thus $\inv{B}\inv{A}$ is an inverse of $AB$.
		\item (Exercise 3.3 15)
		\begin{align*}
		A^T(\inv{A})^T &= (\inv{A}A)^T = I^T = I \\
		(\inv{A})^TA^T &= (A\inv{A})^T = I^T = I
		\end{align*} thus $(\inv{A})^T$ is an inverse of $A^T$.
		\item (i) $\inv{(A^0)} = \inv{I} = I = (\inv{A})^0$
		
		\noindent (ii) Suppose that for arbitrary nonnegative integer $n$, $\inv{(A^n)} = (\inv{A})^n$. Then \begin{align*}
		\inv{(A^{n+1})} = \inv{(A^nA)} = \inv{A}\inv{(A^n)} = \inv{A}(\inv{A})^n = (\inv{A})^{n+1}
		\end{align*}
		By (i) and (ii), $\inv{(A^n)} = (\inv{A})^n$ for any nonnegative integer $n$.
	\end{enumerate}
\end{proof}

\noindent \textit{Definition.} An \textbf{elementary matrix} is any matrix that can be obatined by performing \textit{single} elementary row operation on an identity matrix.

\noindent \\ \textbf{Types of Elementary Matrices}

\noindent Let $E$ be an $n \times n$ matrix. Then $E = \begin{bmatrix}
\textbf{E}^R_1 \\ \textbf{E}^R_2 \\ \vdots \\ \textbf{E}^R_n
\end{bmatrix}$ is an elementary matrix if and only if $E$ satisfies at least one of the followings:
\begin{enumerate}
	\item There exist integers $p, q$ such that $1 \le p, q \le n$ and \begin{align*}
	\textbf{E}^R_i = \begin{cases}
	\textbf{e}_q &\mbox{ if } i=p \\
	\textbf{e}_p &\mbox{ if } i=q \\
	\textbf{e}_i &\mbox{ otherwise } 
	\end{cases}
	\end{align*}
	\item There exists an integer $p$ and a nonzero scalar $k$ such that $1 \le p \le n$ and
	\begin{align*}
	\textbf{E}^R_i = \begin{cases}
	k\textbf{e}_p &\mbox{ if } i=p \\
	\textbf{e}_i &\mbox{ otherwise }
	\end{cases}
	\end{align*}
	\item There exists integers $p, q$ and a scalar $k$ such that $1 \le p, q \le n$ and \begin{align*}
	\textbf{E}^R_i = \begin{cases}
	\textbf{e}_p + k\textbf{e}_q &\mbox{ if } i=p \\
	\textbf{e}_i &\mbox{ otherwise }
	\end{cases}
	\end{align*}
\end{enumerate}

\begin{theorem}
	If a elementary row operation converts $I_n$ to $E$, then the same operation converts $n \times r$ matrix $A$ to $EA$.
\end{theorem}

\begin{proof}
	Let $A$ be a $n \times r$ matrix, $k$ be a nonzero scalar, $p, q$ be integers between 1 and $n$, and $E$ be an elementary matrix which corresponds to each type of elementary row operation.
	\begin{align*}
	A = \begin{bmatrix}
	\textbf{A}^R_1 \\ \vdots \\ \textbf{A}^R_p \\ \vdots \\ \textbf{A}^R_q \\ \vdots \\ \textbf{A}^R_n
	\end{bmatrix} &\xrightarrow{R_p \leftrightarrow R_q} \begin{bmatrix}
	\textbf{A}^R_1 \\ \vdots \\ \textbf{A}^R_q \\ \vdots \\ \textbf{A}^R_p \\ \vdots \\ \textbf{A}^R_n  
	\end{bmatrix} = \begin{bmatrix}
	\textbf{e}_1A \\ \vdots \\ \textbf{e}_qA \\ \vdots \\ \textbf{e}_pA \\ \vdots \\ \textbf{e}_nA
	\end{bmatrix} = \begin{bmatrix}
	\textbf{e}_1 \\ \vdots \\ \textbf{e}_q \\ \vdots \\ \textbf{e}_p \\ \vdots \\ \textbf{e}_n
	\end{bmatrix}A = EA \\
	A = \begin{bmatrix}
	\textbf{A}^R_1 \\ \vdots \\ \textbf{A}^R_p \\ \vdots \\ \textbf{A}^R_n
	\end{bmatrix} &\xrightarrow{kR_p} \begin{bmatrix}
	\textbf{A}^R_1 \\ \vdots \\ k\textbf{A}^R_p \\ \vdots \\ \textbf{A}^R_n
	\end{bmatrix} = \begin{bmatrix}
	\textbf{e}_1A \\ \vdots \\ k(\textbf{e}_pA) \\ \vdots \\ \textbf{e}_nA
	\end{bmatrix} = \begin{bmatrix}
	\textbf{e}_1 \\ \vdots \\ k\textbf{e}_p \\ \vdots \\ \textbf{e}_n
	\end{bmatrix}A = EA \\
	A = \begin{bmatrix}
	\textbf{A}^R_1 \\ \vdots \\ \textbf{A}^R_p \\ \vdots \\ \textbf{A}^R_n
	\end{bmatrix} &\xrightarrow{R_p + kR_q} \begin{bmatrix}
	\textbf{A}^R_1 \\ \vdots \\ \textbf{A}^R_p + k\textbf{A}^R_q \\ \vdots \\ \textbf{A}^R_n
	\end{bmatrix} = \begin{bmatrix}
	\textbf{e}_1A \\ \vdots \\ \textbf{e}_pA + k(\textbf{e}_qA) \\ \vdots \\ \textbf{e}_nA
	\end{bmatrix} = \begin{bmatrix}
	\textbf{e}_1 \\ \vdots \\ \textbf{e}_p + k\textbf{e}_q \\ \vdots \\ \textbf{e}_n
	\end{bmatrix}A = EA
	\end{align*}
\end{proof}

\begin{theorem}
	All elementary matrices are invertible, and the inverse of an elementary matrix is also an elementary matrix.
\end{theorem}

\begin{proof}
	Let $E$ be an elementary matrix. Since there exists a reverse operation for the elementary row operations corresponds to $E$, the elementary matrix $E'$ of the reverse operation will satisfy $EE' = E'E = I$ by Theorem 3.10. Therefore, the inverse of $E$ exists and the inverse is also an elementary matrix.
\end{proof}

\begin{theorem}[The Fundamental Theorem of Invertible Matrices - Version 1]
	Let $A$ be an $n \times n$ matrix. Then the following propositions are equivalent;
	\begin{enumerate}
		\item $A$ is invertible.
		\item $A\textbf{x} = \textbf{b}$ has a unique solution for every $\textbf{b} \in$ \Rn.
		\item $A\textbf{x} = \textbf{0}$ has only the trivial solution.
		\item The RREF of $A$ is $I_n$.
		\item $A$ is a product of elementary matrices.
	\end{enumerate}
\end{theorem}

\begin{proof}
	We give the proof for this theorem at Version 2.
\end{proof}

\begin{theorem}
	Let $A$ be a square matrix. If $B$ is a square matrix such that either $AB = I$ or $BA = I$, then $A$ is invertible and $\inv{A} = B$.
\end{theorem}

\begin{proof}
	(i) Suppose that $BA = I$. Then the linear system $A\textbf{x} = \textbf{0}$ has only the trivial solution, since \begin{align*}
	\textbf{x} = I\textbf{x} = (BA)\textbf{x} = B(A\textbf{x}) = \textbf{0}
	\end{align*}
	Thus $A$ is invertible by F.T.I.M. Then \begin{align*}
	BA = I \Rightarrow (BA)\inv{A} = \inv{A} \Rightarrow B(A\inv{A}) = B = \inv{A}
	\end{align*} Therefore $B$ is an inverse of $A$.
	
	\noindent (ii) (Exercise 3.3 41) Suppose that $AB = I$. By (i), $B$ is invertible and $\inv{B} = A$. Therefore, $A$ is also invertible and $\inv{A} = \inv{(\inv{B})} = B$.
\end{proof}

\begin{theorem}
	Let $A$ be a square matrix. Then a sequence of elementary row operations which converts $A$ to $I$ also converts $I$ to $\inv{A}$.
\end{theorem}

\begin{proof}
	Let $E_1, E_2, \cdots, E_k$ be the elementary matrices which correspond to each step of elementary row operation converting $A$ to $I$. Then by Theorem 3.10, \begin{align*}
	I = (E_k\cdots E_2E_1)A
	\end{align*} Then by Theorem 3.13, \begin{align*}
	\inv{A} = E_k\cdots E_2E_1 = (E_k\cdots E_2E_1)I
	\end{align*} Therefore, the same steps of elementary row operation also converts $I$ to $\inv{A}$.
\end{proof}

\section{Subspaces, Basis, Dimension, and Rank}

\textit{Definition.} Let $S$ be a set of vectors in \Rn. Then $S$ is a \textbf{subspace} of \Rn if \begin{enumerate}
	\item $\textbf{0} \in S$
	\item $\textbf{u}, \textbf{v} \in S \Rightarrow \textbf{u}+\textbf{v} \in S$
	\item $\textbf{u} \in S \Rightarrow c\textbf{u} \in S$ for any scalar $c$
\end{enumerate}

\noindent \textit{Definition.} Let $A$ be an $n \times m$ matrix.

\noindent The \textbf{row space} of $A$, denoted by row($A$), is a subspace of \Rm spanned by the rows of $A$.

\noindent The \textbf{column space} of $A$, denoted by col($A$), is a subspace of \Rn spanned by the columns of $A$.

\noindent The \textbf{null space} of $A$, denoted by null($A$), is a subspace of \Rm consisting of solutions of the homogeneous linear system $A\textbf{x} = \textbf{0}$.  

\begin{theorem}
	For vectors \vk in \Rn, span( \vk) is a subspace of \Rn.	
\end{theorem}

\begin{proof}
	Let \vk be vectors in \Rn, and let $S$ = span( \vk).
	
	\noindent (i) Since $0\textbf{v}_1 + 0\textbf{v}_2 + \cdots + 0\textbf{v}_k = \textbf{0}$, $\textbf{0} \in S$.
	
	\noindent (ii) Let $\textbf{u}$ and $\textbf{v}$ be vectors in $S$. Then there exist scalars $c_1, c_2 \cdots, c_k$ and $d_1, d_2, \cdots, d_k$ such that \begin{align*}
	\textbf{u} &= c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_k\textbf{v}_k \\
	\textbf{v} &= d_1\textbf{v}_1 + d_2\textbf{v}_2 + \cdots + d_k\textbf{v}_k
	\end{align*} Then \begin{align*}
	\textbf{u} + \textbf{v} &= (c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_k\textbf{v}_k) + (d_1\textbf{v}_1 + d_2\textbf{v}_2 + \cdots + d_k\textbf{v}_k) \\ &= (c_1+d_1)\textbf{v}_1 + \cdots + (c_k+d_k)\textbf{v}_k
	\end{align*} Therefore $\textbf{u}+\textbf{v} \in S$.
	
	\noindent (iii) Let $\textbf{u}$ be a vector in $S$. Then there exist scalars $c_1, c_2, \cdots, c_k$ such that \begin{align*}
	\textbf{u} &= c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_k\textbf{v}_k
	\end{align*} Then for all scalar $c$, \begin{align*}
	c\textbf{u} &= c(c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_k\textbf{v}_k) \\
	&= cc_1\textbf{v}_1 + cc_2\textbf{v}_2 + \cdots + cc_k\textbf{v}_k
	\end{align*} Therefore $c\textbf{u} \in S$.
	
	\noindent By (i), (ii), and (iii), $S$ is a subspace of \Rn.
\end{proof}

\begin{theorem}
	If matrices $A$ and $B$ are row equivalent, then row($A$) = row($B$).
\end{theorem}

\begin{proof}
	Let $A$ and $B$ be $n \times m$ matrices. Since $A$ and $B$ are row equivalent, there exist $n \times n$ elementary matrices $E_1, E_2, \cdots ,E_k$ which satisfies \begin{align*}
	B = (E_k\cdots E_2E_1)A
	\end{align*} by Theorem 3.10. Let $C = E_k\cdots E_2E_1$.
	
	\noindent Since $B = CA$, \begin{align*}
	B = \begin{bmatrix}
	\textbf{C}^R_1 \\ \vdots \\ \textbf{C}^R_n
	\end{bmatrix}A = \begin{bmatrix}
	\textbf{C}^R_1A \\ \vdots \\ \textbf{C}^R_nA
	\end{bmatrix} = \begin{bmatrix}
	C_{11}\textbf{A}^R_1 + \cdots + C_{1n}\textbf{A}^R_n \\ \vdots \\
	C_{n1}\textbf{A}^R_1 + \cdots + C_{nn}\textbf{A}^R_n
	\end{bmatrix}
	\end{align*} Thus, rows of $B$ are linear combinations of rows of $A$. Also, since $C$ is invertible by Theorem 3.11, $A = \inv{C}B$, so rows of $A$ are linear combinations of rows of $B$. Therefore, by the proposition at Exercise 2.3 21(b), span($\textbf{A}^R_1, \cdots, \textbf{A}^R_n$) = span($\textbf{B}^R_1, \cdots, \textbf{B}^R_n$), that is row($A$) = row($B$).
\end{proof}

\begin{theorem}
	Let $A$ be an $n \times m$ matrix and let $N$ be the set of solutions of the homogeneous linear system $A\textbf{x} = \textbf{0}$. Then $N$ is a subspace of \Rm.
\end{theorem}

\begin{proof}
	Let $A$ be an $n \times m$ matrix and let $N$ be the set of all solutions of $A\textbf{x} = \textbf{0}$.
	
	\noindent (i) Since $A\textbf{0} = \textbf{0}$, $\textbf{0} \in N$.
	
	\noindent (ii) Let $\textbf{u}, \textbf{v}$ be vectors in $N$. Since $A\textbf{u} = A\textbf{v} = \textbf{0}$, $A(\textbf{u}+\textbf{v}) = A\textbf{u} + A\textbf{v} = \textbf{0}$. Therefore, $\textbf{u} + \textbf{v} \in N$.
	
	\noindent (iii) Let $\textbf{u}$ be a vector in $N$. Since $A\textbf{u} = \textbf{0}$, for all scalar $c$, $A(c\textbf{u}) = c(A\textbf{u}) = c\textbf{0} = \textbf{0}$. Therefore, $c\textbf{u} \in N$.
	
	\noindent By (i), (ii), and (iii), $N$ is a subspace of \Rn.
\end{proof}

\begin{theorem}
	For any system of linear equations $A\textbf{x} = \textbf{b}$ with real coefficients, exactly one of the following is true; \begin{enumerate}
		\item The system has no solution.
		\item The system has a unique solution.
		\item The system has infinitely many solutions.
	\end{enumerate}
\end{theorem}

\begin{proof}
	Consider the case when the system of linear equations has more than two solutions. Suppose that $\textbf{x}_1, \textbf{x}_2 \in$ \Rn are solutions of the linear system $A\textbf{x} = \textbf{b}$. Then $A(\textbf{x}_2 - \textbf{x}_1) = \textbf{0}$, so $\textbf{x}_2 - \textbf{x}_1 \in$ null($A$). By the definition of subspace, for any scalar $c$, $c(\textbf{x}_2 - \textbf{x}_1) \in$ null($A$), so $A(c(\textbf{x}_2 - \textbf{x}_1)) = \textbf{0}$. Since $A(\textbf{x}_1 + c(\textbf{x}_2 - \textbf{x}_1)) = A\textbf{x}_1 + A(c(\textbf{x}_2 - \textbf{x}_1)) = \textbf{b} + \textbf{0} = \textbf{b}$, there exist infinitely many solutions.
\end{proof}

\noindent \textit{Definition.} A \textbf{basis} for a subspace $S$ in \Rn is a set of vectors in $S$ that \begin{enumerate}
	\item spans $S$
	\item is linearly independent.
\end{enumerate}

\noindent \textbf{How to Find Bases of Subspaces Related to Matrix}
\begin{enumerate}
	\item The basis of row($A$) consists of the nonzero rows of RREF of the matrix.
	\item The basis of col($A$) consists of the columns of $A$ where the leading 1s of $A$ are located.
	\item The basis of null($A$) consists of the vectors which spans the solution of $A\textbf{x} = \textbf{0}$.
\end{enumerate} 

\noindent \textbf{How to Find Subspaces Related to Matrix : Example}

\noindent Find bases of row($A$), col($A$), and null($A$) where \begin{align*}
A = \begin{bmatrix}
1 & 1 & 3 & 1 & 6 \\ 2 & -1 & 0 & 1 & -1 \\
-3 & 2 & 1 & -2 & 1 \\ 4 & 1 & 6 & 1 & 3
\end{bmatrix}
\end{align*}

\noindent \textit{Solution.} The RREF of $A$ is \begin{align*}
R = \begin{bmatrix}
1 & 0 & 1 & 0 & -1 \\ 0 & 1 & 2 & 0 & 3 \\
0 & 0 & 0 & 1 & 4 \\ 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\end{align*}
The basis of row($A$) is \begin{align*}
\left\{ \begin{bmatrix}
1 & 0 & 1 & 0 & -1
\end{bmatrix}, \begin{bmatrix}
0 & 1 & 2 & 0 & 3
\end{bmatrix}, \begin{bmatrix}
0 & 0 & 0 & 1 & 4
\end{bmatrix} \right\}
\end{align*}
The basis of col($A$) consists of the first, second, and fourth columns of $A$, so the basis of col($A$) is \begin{align*}
\left\{ \begin{bmatrix}
1 \\ 2 \\ -3 \\ 4
\end{bmatrix}, \begin{bmatrix}
1 \\ -1 \\ 2 \\ 1
\end{bmatrix}, \begin{bmatrix}
1 \\ 1 \\ -2 \\ 1
\end{bmatrix} \right\}
\end{align*}
The solution of $A\textbf{x} = \textbf{0}$ is \begin{align*}
\textbf{x} = s\begin{bmatrix}
-1 \\ -2 \\ 1 \\ 0 \\ 0
\end{bmatrix} + t\begin{bmatrix}
1 \\ -3 \\ 0 \\ -4 \\ 1
\end{bmatrix} (s,t \in \mathbb{R})
\end{align*}
thus the basis of null($A$) is \begin{align*}
\left\{
\begin{bmatrix}
-1 \\ -2 \\ 1 \\ 0 \\ 0
\end{bmatrix}, \begin{bmatrix}
1 \\ -3 \\ 0 \\ -4 \\ 1
\end{bmatrix}
\right\}
\end{align*}

\begin{theorem}
	Let $S$ be a subspace of \Rn. Then any two bases for $S$ have the same number of vectors.
\end{theorem}

\begin{proof}
	Let $\mathcal{B}_1 = \{\textbf{u}_1, \cdots, \textbf{u}_k\}$ and $\mathcal{B}_2 = \{\textbf{v}_1, \cdots, \textbf{v}_m \}$ be bases for subspace $S$ of \Rn.
	
	\noindent Suppose that $k > m$. Since $\mathcal{B}_1 \subset S$ and $\mathcal{B}_2$ is a basis for $S$, there exist scalars $a_{ij}$ such that \begin{align*}
	\textbf{u}_1 &= a_{11}\textbf{v}_1 + \cdots + a_{1m}\textbf{v}_m \\
	&\vdots \\
	\textbf{u}_k &= a_{k1}\textbf{v}_1 + \cdots + a_{km}\textbf{v}_m
	\end{align*} Since $\mathcal{B}_2$ is a basis for $S$, $\textbf{v}_1, \cdots, \textbf{v}_m$ are linearly independent. So the linear system \begin{align*}
	&c_1\textbf{u}_1 + \cdots + c_k\textbf{u}_k \\ &= (c_1a_{11} + \cdots + c_ka_{k1})\textbf{v}_1 + (c_1a_{12} + \cdots + c_ka_{k2})\textbf{v}_2 + \cdots + (c_1a_{1m} + \cdots + c_ka_{km})\textbf{v}_m = \textbf{0}
	\end{align*} has only the trivial solution. Thus, the homogeneous linear system with augmented matrix \begin{align*}
	\begin{bmatrix}[ccc|c]
	a_{11} & \cdots & a_{k1} & 0 \\
	\vdots & & \vdots & \vdots \\
	a_{1m} & \cdots & a_{km} & 0
	\end{bmatrix}
	\end{align*} is consistent, and the system has infinitely many solutions because the size of the matrix is $m \times k$. There exists a nontrivial solution of this system, so there exist scalars $c_1, \cdots, c_k$ such that at least one of $c_1, \cdots, c_k$ are nonzero, and $c_1\textbf{u}_1 + \cdots + c_k\textbf{u}_k = \textbf{0}$. Therefore, $\textbf{u}_1, \cdots, \textbf{u}_k$ are linearly dependent, which contradicts with the assumption.
	
	\noindent Similarly, in case of $k < m$ also comes up to contradiction. Therefore, $k = m$.
\end{proof}

\noindent \textit{Definition.} If $S$ is a subspace of \Rn, then the number of vectors in a basis for $S$ is the \textbf{dimension} of $S$, denoted by dim $S$.

\begin{theorem}
	For any matrix $A$,
	\begin{align*}
	\textnormal{dim row}(A) = \textnormal{dim col}(A)
	\end{align*}
\end{theorem}

\begin{proof}
	The number of nonzero rows and the number of leading 1s are the same. Since the nonzero rows form the basis for row($A$), and the columns of $A$ with the leading 1s form the basis for col($A$), dim row($A$) = dim col($A$).
\end{proof}

\noindent \textit{Definition.} The \textbf{rank} of a matrix $A$, denoted by rank($A$), is the dimension of its row and column spaces.

\begin{theorem}
	For any matrix $A$,
	\begin{align*}
	\textnormal{rank}(A) = \textnormal{rank}(A^T)
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
	\mbox{rank}(A) = \mbox{dim row}(A) = \mbox{dim col}(A) = \mbox{dim row}(A^T) = \mbox{rank}(A^T)
	\end{align*}
\end{proof}

\noindent \textit{Definition.} The \textbf{nullity} of a matrix $A$, denoted by nullity($A$), is the dimension of its null space.

\begin{theorem}[The Rank Theorem]
	If $A$ is an $m \times n$ matrix, then \begin{align*}
	\textnormal{rank}(A) + \textnormal{nullity}(A) = n
	\end{align*}
\end{theorem}

\begin{proof}
	By Theorem 2.2.
\end{proof}

\begin{theorem} [The Fundamental Theorem of Invertible Matrices : Version 2]
	Let $A$ be an $n \times n$ matrix. The following propositions are equivalent;
	\begin{enumerate}
		\item $A$ is invertible.
		\item $A\textbf{x} = \textbf{b}$ has a unique solution for every $\textbf{b} \in$ \Rn.
		\item $A\textbf{x} = \textbf{0}$ has only the trivial solution.
		\item The RREF of $A$ is $I_n$.
		\item $A$ is a product of elementary matrices.
		\item rank($A$) = $n$
		\item nullity($A$) = $0$
		\item The columns of $A$ are linearly independent.
		\item The columns of $A$ span \Rn.
		\item The columns of $A$ form a basis for \Rn.
		\item The rows of $A$ are linearly independent.
		\item The rows of $A$ span \Rn.
		\item The rows of $A$ form a basis for \Rn.
	\end{enumerate}
\end{theorem}

\begin{proof}
	Let $A$ be an $n \times n$ matrix.
	
	\noindent (a $\Rightarrow$ b) Theorem 3.7
	
	\noindent \\ (b $\Rightarrow$ c) Since $\textbf{x} = \textbf{0}$ is a solution of $A\textbf{x} = \textbf{0}$, $\textbf{0}$ is the unique solution of the system. Therefore, the system $A\textbf{x} = \textbf{0}$ has only the trivial solution.
	
	\noindent \\ (c $\Rightarrow$ d) The corresponding augmented martrix of the homogeneous system with unique solution $\textbf{0}$ is $[I_n | \textbf{0}]$. Since $A\textbf{x}=\textbf{0}$ has only the unique solution $\textbf{0}$, $A$ is row equivalent with $I_n$, therefore RREF of $A$ is $I_n$.
	
	\noindent \\ (d $\Rightarrow$ e)
	Since $A$ and $I_n$ are row equivalent, there exists a sequence of elementary row operations which converts $I_n$ to $A$. Let $E_1, E_2, \cdots, E_k$ be the corresponding elementary matrices to each steps fo elementary row operations. Then by Theorem 3.10, \begin{align*}
	A = E_{k}\cdots E_2E_1I_n = E_{k}\cdots E_2E_1
	\end{align*} Therefore $A$ is a product of elementary matrices.
	
	\noindent \\ (e $\Rightarrow$ a)
	Since all elementary matrices are invertible, $A$, which is the product of elementary matrices, is also invertible.
	
	\noindent \\ (d $\Leftrightarrow$ f)
	If the RREF of $A$ is $I_n$, since rank($I_n$) = $n$, by Theorem 3.16 rank($A$) = rank($I_n$) = $n$. Conversely, if rank($A$) = $n$, the RREF of $A$ has $n$ leading 1s so the RREF should be $I_n$.
	
	\noindent \\ (f $\Leftrightarrow$ g)
	Rank Theorem.
	
	\noindent \\ (c $\Leftrightarrow$ h)
	Let $A$ be an $n \times m$ matrix. Since $A\textbf{x} = \textbf{0}$ has only the trivial solution,  \begin{align*}
	c_1\textbf{A}^C_1 + \cdots + c_m\textbf{A}^C_m = \textbf{0}
	\end{align*} is true only if $c_1 = \cdots = c_m = 0$. Therefore, columns of $A$ are linearly independent.
	
	\noindent Conversely, if columns of $A$ are linearly independent, the system $c_1\textbf{A}^C_1 + \cdots + c_m\textbf{A}^C_m = \textbf{0}$ does not have a nontrivial solution, so $A\textbf{x} = \textbf{0}$ has only the trivial solution. 
	
	\noindent \\ (b $\Rightarrow$ i)
	Since for any $\textbf{b} \in$ \Rn, $A\textbf{x} = \textbf{b}$ is consistent, $\textbf{b}$ is a linear combination of colmuns of $A$. Therefore, the columns of $A$ span \Rn.
	
	\noindent \\ (i $\Rightarrow$ f)
	col($A$) = \Rn, so rank($A$) = dim col($A$) = dim \Rn = $n$.
	
	\noindent \\ (h and i $\Leftrightarrow$ j)
	Definition of basis for a subspace of \Rn.
	
	\noindent \\ By Theorem 3.21, rank($A^T$) = rank($A$). Therefore, the proposition related to columns of $A$ can be applied to rows of $A$, so (k), (l), and (m) are also equivalent.
	
\end{proof}

\begin{theorem}
	Let $A$ be an $n \times m$ matrix. Then \begin{enumerate}
		\item rank($A^TA$) = rank($A$)
		\item The $n \times n$ matrix $A^TA$ is invertible if and only if rank($A$) = $n$.
	\end{enumerate}
\end{theorem}

\begin{proof}
	\begin{enumerate}
		\item Let $A$ be an $n \times m$ matrix.
		
		\noindent (i) Suppose $\textbf{x}$ is a solution for $A\textbf{x} = \textbf{0}$. Then $A^TA\textbf{x} = \textbf{0}$. Therefore null($A$) $\subset$ null($A^TA$).
		
		\noindent (ii) Suppose $\textbf{x}$ is a solution for $A^TA\textbf{x} = \textbf{0}$. Then \begin{align*}
			(A\textbf{x}) \cdot (A\textbf{x}) = (A\textbf{x})^T(A\textbf{x}) = \textbf{x}^TA^TA\textbf{x} = \textbf{x}^T\textbf{0} = 0
		\end{align*} Therefore, $A\textbf{x} = \textbf{0}$, so null($A^TA$) $\subset$ null($A$).
		
		\noindent By (i), (ii), null($A$) = null($A^TA$), so nullity($A$) = nullity($A^TA$), thus rank($A$) = rank($A^TA$) by Rank Theorem.
		
		\item Let $A$ be an $n \times n$ matrix. Since rank($A$) = rank($A^TA$), by F.T.I.M, $A^TA$ is invertible if and only if rank($A^TA$) = rank($A$) = $n$.
		
	\end{enumerate}
\end{proof}

\begin{theorem}
	Let $S$ be a subspace of \Rn and let $\mathcal{B} = \{$ \vk $\}$ be a basis for $S$. For every vector $\textbf{v}$ in $S$, there is exactly one way to represent $\textbf{v}$ as a linear combination of the basis vectors in $\mathcal{B}$.
\end{theorem}

\begin{proof}
	(Existence) Since $\mathcal{B}$ is a basis of $S$, any vector $\textbf{v} \in S$ is a linear combination of vectors in $\mathcal{B}$.
	
	\noindent  \\ (Uniqueness) Suppose that there are more than two ways to represent $\textbf{v} \in S$ as a linear combination of $\mathcal{B} = \{$\vk $\}$. That is, the linear system with augmented matrix [$A \vert \textbf{v}$] = $\begin{bmatrix}[ccc|c]
		\textbf{v}_1 & \cdots & \textbf{v}_k & \textbf{v}
	\end{bmatrix}$ has more than two solutions. Let those two solutions be $\textbf{x}_1$ and $\textbf{x}_2$, then $A\textbf{x}_1 = \textbf{v}$ and $A\textbf{x}_2 = \textbf{v}_2$. Since $A(\textbf{x}_1 - \textbf{x}_2) = \textbf{0}$, the homogeneous system $A\textbf{x} = \textbf{0}$ has a nontrivial solution, which contradicts with F.T.I.M. Therefore, representation of $\textbf{v}$ as a linear combination of $\mathcal{B}$ is unique.
\end{proof}

\noindent \textit{Definition.} Let $S$ be a subspace of \Rn and let $\mathcal{B}$  = \{\vk\} be a basis for $S$. Let $\textbf{v}$ be a vector in $S$, and $\textbf{v} = c_1\textbf{v}_1 + c_2\textbf{v}_2 + \cdots + c_k\textbf{v}_k$. Then $c_1, c_2, \cdots, c_k$ are the \textbf{coordinates of $\textbf{v}$ with respect to $\mathcal{B}$}, and the column vector \begin{align*}
[\textbf{v}]_\mathcal{B} = \begin{bmatrix}
c_1 \\ c_2 \\ \vdots \\ c_k
\end{bmatrix}
\end{align*} is called the \textbf{coordinate vector of $\textbf{v}$ with respect ot $\mathcal{B}$}.

\begin{plaintheorem}[Inequality on subspaces' dimension]
	\textit{Note.} This lemma will be used in this subsection as `Lemma'. \\
	Let $ S_{1} $, $ S_{2} $ be subspaces of $ \mathbb{R}^{n} $. If $ S_{1} \subset S_{2} $, then $ \text{dim}(S_{1}) \le \text{dim}(S_{2})  $
\end{plaintheorem}
\begin{proof}
	Let $ \mathcal{B}_{1} = \{\textbf{u}_1, \textbf{u}_2, \cdots \textbf{u}_k\} $ be a basis for $ S_{1} $, $ \mathcal{B}_{2} = \{\textbf{v}_1, \textbf{v}_2, \cdots \textbf{v}_m\} $ be a basis for $ S_{2} $ . 
	Suppose $ k>m $. Since $ \mathcal{B} \subset S_{1} \subset S_{2} $, there exist $ a_{ij} $ ($ 1\leq i \leq k $, $ 1 \leq j \leq m $) such that 
	\begin{gather*}
	\textbf{u}_1 = a_{11}\textbf{v}_1+ \cdots + a_{1m}\textbf{v}_m \\
	\vdots \\
	\textbf{u}_k = a_{k1}\textbf{v}_1+ \cdots + a_{km}\textbf{v}_m \\
	\end{gather*}
	Then $ c_1\textbf{u}_1+c_2\textbf{u}_2+\cdots c_k\textbf{u}_k = \left(c_{1}a_{11}+\cdots + c_{k}a_{k1}\right)\textbf{v}_1 + \cdots \left(c_{1}a_{1m}+\cdots + c_{k}a_{km}\right)\textbf{v}_m = \textbf{0}$ has only the trivial solution $ c_{1}a_{11}+\cdots + c_{k}a_{k1} \cdots c_{1}a_{1m}+\cdots + c_{k}a_{km} = 0$ since $ \mathcal{B}_{2} $ is linearly independent.
	
	$\begin{bmatrix}[ccc|c]
	a_{11} & \cdots & a_{k1} & 0 \\
	\vdots & & \vdots & \vdots \\
	a_{1m} & \cdots & a_{km} & 0 \\
	\end{bmatrix}$ has a nontrivial solution since 
	$ \text{rank}\left(\begin{bmatrix}[ccc|c]
	a_{11} & \cdots & a_{k1} & 0 \\
	\vdots & & \vdots & \vdots \\
	a_{1m} & \cdots & a_{km} & 0 \\
	\end{bmatrix}\right) \leq m < k$. 
	The number of free variables is $ k-\text{rank}\left(\begin{bmatrix}[ccc|c]
	a_{11} & \cdots & a_{k1} & 0 \\
	\vdots & & \vdots & \vdots \\
	a_{1m} & \cdots & a_{km} & 0 \\
	\end{bmatrix}\right) > 0 $. \\
	Therefore, there exist scalars $ c_{1}, \cdots, c_{k} $ such that at least one of $ c_{1}, \cdots, c_{k} $ are nonzero and $ c_{1}\textbf{u}_{1} + \cdots + c_{k}\textbf{u}_{k} = \textbf{0}$, so $ \textbf{u}_{1}, \cdots , \textbf{u}_{k} $ are linearly dependent. \\
	However, since $ \mathcal{B}_{1} $ is a basis, $ \textbf{u}_{1}, \cdots , \textbf{u}_{k} $ should be linearly independent : contradiction.
	$ \therefore k\leq m $, that is, $ \text{dim} S_{1} \leq \text{dim} S_{2} $.
\end{proof}

\section{Solutions of Exercises Worthy to Solve}

% Typed by 14041 

\begin{enumerate}
	\item \textbf{Exercise 3.5 57}
	\begin{proof}
		Let $ A $ be an $ m\times n $ matrix and let $ R $ be the RREF of A. Then for any $ \textbf{v} \in $ null($A$) , 
		$ R\textbf{v} = \begin{bmatrix}
		\textbf{R}^R_1 \cdot\textbf{v} \\
		\vdots \\
		\textbf{R}^R_n \cdot\textbf{v} \\
		\end{bmatrix} = \textbf{0}$. $ \therefore \textbf{R}^R_i\cdot\textbf{v}=\textbf{0} $ for $ 1\leq i \leq n $.
		For every $ \textbf{n} \in $ row($A$) , $ \exists c_1,c_2, \cdots c_n $ such that $ \textbf{u}=c_1\textbf{R}^R_1 + c_2\textbf{R}^R_2 + \cdots c_n\textbf{R}^R_n $ 
		since $ \{\textbf{R}^R_1, \cdots \textbf{R}^R_n \} $ form a basis for row($A$), 
		then 
		\begin{align*}
		\textbf{u}\cdot\textbf{v} &= \left(c_1\textbf{R}^R_1 + c_2\textbf{R}^R_2 + \cdots c_n\textbf{R}^R_n\right)\cdot \textbf{v}  \\
		&= c_1\textbf{R}^R_1\cdot\textbf{v} + c_2\textbf{R}^R_2\cdot\textbf{v} + \cdots c_n\textbf{R}^R_n\cdot\textbf{v} = 0\\
		\end{align*}
		Therefore, for any $ \textbf{u} \in $ row($A$) and $ \textbf{v} \in $ null($A$), $ \textbf{u}\cdot \textbf{v}=0 $ so $ \textbf{u} $ and $ \textbf{v} $ are orthogonal.
	\end{proof}
	\item \textbf{Exercise 3.5 58}
	\begin{proof}
		Let $A$, $ B $ be $ n\times n $ matrices with rank $ n $.
		Then by F.T.I.M, $ A\textbf{x}=\textbf{0} $ and $ B\textbf{x}=\textbf{0} $ has only the trivial solution, and $ \exists \inv{A} $, $ \exists \inv{B} $. \\
		Suppose that $ AB\textbf{x}=\textbf{0} $ has a nontrivial solution $ \textbf{x}_1\neq \textbf{0} $. Then $ AB\textbf{x}_1=\textbf{0} $, so $ \inv{A}AB\textbf{x}_1=B\textbf{x}_1=\textbf{0} $, so the system $ B\textbf{x}=\textbf{0} $ has a nontrivial solution. Since this contradict with our assumption, $ (AB)\textbf{x}=\textbf{0} $ has only the trivial solution. \\
		By F.T.I.M, rank of $ AB $ is $ n $.
	\end{proof}
	Another solution : 
	\begin{proof}
		By F.T.I.M, $ \exists \inv{A}$ and $ \exists \inv{B}$. Therefore, $ \inv{AB}=\inv{B}\inv{A} $. By F.T.I.M, rank($ AB $) $ =n $.
	\end{proof}
	\item \textbf{Exercise 3.5 59}
	\begin{proof}
		\noindent \textbf{(a)} For $ n\times m $ matrix $ A $ and $ m\times r $ matrix $ B $,\\ $ AB = \begin{bmatrix}
		\textbf{A}^R_1 \\
		\vdots \\
		\textbf{A}^R_n \\
		\end{bmatrix}B = \begin{bmatrix}
		\textbf{A}^R_1 B \\
		\vdots \\
		\textbf{A}^R_n B \\
		\end{bmatrix} = \begin{bmatrix}
		A_{11}\textbf{B}^R_1 + \cdots + A_{1m}\textbf{B}^R_m
		\vdots
		A_{n1}\textbf{B}^R_1 + \cdots + A_{nm}\textbf{B}^R_m
		\end{bmatrix}$. \\
		Since rows of $ AB $ are linear combinations of $ \textbf{B}^R_1, \cdots , \textbf{B}^R_m $, $ \text{row}(AB) \subset \text{row}(B) $. \\
		$ \therefore $ By Lemma,$ \text{rank}(AB) \leq \text{rank}(B) $.
		\noindent \textbf{(b)} In case when $ A=O $.
	\end{proof}
	Another solution of \textbf{(a)} : 
	\begin{proof}
		For $ n\times m $ matrix $ A $ and $ m\times r $ matrix $ B $,
		\begin{align*}
		\text{rank}(B) &= r - \text{nullity}(B) \\
		\text{rank}(AB) &= r - \text{nullity}(AB) \\
		\end{align*}
		$ \forall \textbf{x}\in \text{null}(B) $, $ (AB)\textbf{x}=A(B\textbf{x})=A\textbf{0}=\textbf{0} $. \textit{i.e.} $ \textbf{x}\in \text{null}(AB) $. $ \therefore \text{null}(B) \subset \text{null}(AB) $. \\
		By Lemma, $ \text{nullity}(B)\leq \text{nullity}(AB)$. $ \therefore \text{rank}(B) \geq \text{rank}(AB) $.
	\end{proof}
	\item \textbf{Exercise 3.5 60}
	\begin{proof}
		\noindent \textbf{(a)}
		For $ n\times m $ matrix $ A $ and $ m\times r $ matrix $ B $, 
		$ AB=A\begin{bmatrix}
		\textbf{B}^C_1 & \textbf{B}^C_2 & \cdots & \textbf{B}^C_r
		\end{bmatrix} = \begin{bmatrix}
		A\textbf{B}^C_1 & A\textbf{B}^C_2 & \cdots & A\textbf{B}^C_r
		\end{bmatrix} \begin{bmatrix}
		\left(B_{11}\textbf{A}^C_1 + \cdots + B_{m1}\textbf{A}^C_m \right) & \cdots & \left(B_{1r}\textbf{A}^C_1 + \cdots + B_{mr}\textbf{A}^C_m \right)
		\end{bmatrix}$
		Since columns of $ AB $ are linear combinations of $ \textbf{A}^C_1 \cdots \textbf{A}^C_m$, $ \text{col}(AB) \subset \text{col}(A) $. \\
		$ \therefore $ By Lemma, $ \text{rank}(AB) \leq \text{rank}(B) $.
		\noindent \textbf{(b)}
		In case when $ B=O $
	\end{proof}
	Another solution of \textbf{(a)} :
	\begin{proof}
		\begin{align*}
		\text{rank}(AB) &= \text{rank}(B^{T}A^{T}) && \text{(Theorem 3.21)} \\
		&\leq \text{rank}(A^{T}) && \text{(Exercise 3.5 59)} \\
		&= \text{rank}(A) && \text{(Theorem 3.21)} \\
		\end{align*}
	\end{proof}
	\item \textbf{Exercise 3.5 61}
	\begin{proof}
		\noindent \textbf{(a)} 
		By Exercise 3.5 59, $ \text{rank}(A) = \text{rank}\left(\inv{U}\left(UA\right)\right) \leq \text{rank}(UA) \leq \text{rank}(A) $. $ \therefore \text{rank}(UA) = \text{rank}(A) $. 
		
		\noindent \textbf{(b)}
		By Exercise 3.5 60, $ \text{rank}(A) = \text{rank}\left(\left(AV\right)\inv{V}\right) \leq \text{rank}(AV) \leq \text{rank}(A) $. $ \therefore \text{rank}(AV) = \text{rank}(A) $.
	\end{proof}
	\item \textbf{Exercise 3.5 62}
	\begin{proof}
		\noindent ($\Rightarrow$)
		Let $ A $ be an $ m\times n $ matrix with rank 1. Then $ \text{dim}\left(\text{row}(A)\right) = \text{dim}\left(\text{col}(A)\right) = 1$. 
		For $ \textbf{v} \in \mathbb{R}^m$, suppose that $ \{\textbf{v}^T\} $ is a basis for $ \text{row}(A) $. Then $ \exists c_{1}, c_{2}, \cdots , c_{m} $ such that $ \textbf{A}^R_i = c_{i}\textbf{u}^T $. 
		Then $ A = \begin{bmatrix}
		\textbf{A}^R_1 \\
		\vdots \\
		\textbf{A}^R_m \\
		\end{bmatrix} = \begin{bmatrix}
		c_{1}\textbf{v}^T \\
		\vdots \\
		c_{m}\textbf{v}^T \\
		\end{bmatrix} = \begin{bmatrix}
		c_{1} \\
		\vdots \\
		c_{m} \\
		\end{bmatrix}\textbf{v}^T$. Let $ \textbf{u}=\begin{bmatrix}
		c_{1} \\
		\vdots \\
		c_{m} \\
		\end{bmatrix} $, then $ \textbf{u}\textbf{v}^T = A $
		
		\noindent ($\Leftarrow$)
		Suppose that for $ \textbf{u}=\begin{bmatrix}
		u_1 \\
		\vdots \\
		u_m \\
		\end{bmatrix} \in \mathbb{R}^m $ and 
		$ \textbf{v}=\begin{bmatrix}
		v_1 \\
		\vdots \\
		v_m \\
		\end{bmatrix} \in \mathbb{R}^m $, $ \textbf{u}\textbf{v}^T = A$.
		\footnote{  . Nonzero vectors $ \textbf{u} $, $ \textbf{v} $  .} \\
		$ A=\textbf{u}\textbf{v}^T = \textbf{u}\begin{bmatrix}
		v_1 & \cdots & v_n \\
		\end{bmatrix} = \begin{bmatrix}
		v_{1}\textbf{u} & \cdots & v_{n}\textbf{u} \\
		\end{bmatrix} $ \\
		Then $ \{\textbf{u}\} $ forms a basis for $ \text{col}(A) $, since the columns of $ A $ are linear combination of $ \textbf{u} $. \\
		$ \therefore \text{dim}\left(\text{col}(A)\right) = \text{rank}(A) = 1$
	\end{proof}
	\item \textbf{Exercise 3.5 63}
	\begin{proof}
		Let $ A $ be an $ m\times n $ matrix. Then $ A=AI_{n} = \begin{bmatrix}
		\textbf{A}^C_1 & \cdots & \textbf{A}^C_n
		\end{bmatrix} \begin{bmatrix}
		\textbf{e}_1 \\
		\vdots \\
		\textbf{e}_n \\
		\end{bmatrix} = \textbf{A}^C_1\textbf{e}_1 + \cdots + \textbf{A}^C_n\textbf{e}_n $. Since $ \textbf{A}^C_1 \in \mathbb{R}^m $ and $ \textbf{e}_i\in \mathbb{R}^n $, ranks of $ \textbf{A}^C_i\textbf{e}_i $ are all 1.
	\end{proof}
	\item \textbf{Exercise 3.5 64}
	\begin{proof}
		For $ m\times n $ matrices $ A,B $, let $ \mathcal{B}_1 $, $ \mathcal{B}_2 $ be bases for $ \text{row}(A) $,$ \text{row}(B) $. Then $ \left(\textbf{A+B}\right)^R_i = \textbf{A}^R_i + \textbf{B}^R_i $, so $ \left(\textbf{A+B}\right)^R_i $ are linear combinations of vectors in $ \mathcal{B}_1 \cup  \mathcal{B}_2$. \\
		Thus, there exists a subset $ C \subset B $ which forms a basis for $ \text{row}(A+B) $. \\
		$ \therefore \text{rank}(A+B)=\left|C\right| \leq \left|\mathcal{B}_1 \cup \mathcal{B}_2 \right| \leq \left|\mathcal{B}_1\right| + \left|\mathcal{B}_2\right| = \text{rank}(A)+\text{rank}(B) $
	\end{proof}
	\item \textbf{Exercise 3.5 65}
	\begin{proof}
		$ AA = A\begin{bmatrix}
		\textbf{A}^C_1 \cdots \textbf{A}^C_n
		\end{bmatrix} = \begin{bmatrix}
		A\textbf{A}^C_1 \cdots A\textbf{A}^C_n
		\end{bmatrix} = 0$. $ \therefore \textbf{A}^C_1 \cdots \textbf{A}^C_n $ are solutions of the system $ A\textbf{x}=\textbf{0} $. $ \textbf{A}^C_1 \cdots \textbf{A}^C_n \in \text{null}(A) $. and by definition of subspaces, $ \text{col}(A) \subset \text{null}(A) $. \\
		$ \therefore $ By Lemma, $ \text{dim}\left(col(A)\right) = \text{rank}(A) \leq \text{null}(A) $. $ \therefore \text{null}(A) \geq n/2 $
	\end{proof}
	\item \textbf{Exercise 3.5 66}
	\begin{proof}
		\noindent \textbf{(a)}
		$ \left(\textbf{x}^TA\textbf{x}\right)^T = \textbf{x}^T A\textbf{x} = -\textbf{x}^T A \textbf{x} $. $ \therefore $ $ \textbf{x}^T A \textbf{x} $ is also skew-symmetric. Since $ \textbf{x}^T A \textbf{x} $ is a $ 1\times 1 $ matrix and all main diagonal entries are zero in skew-symmetric matrices, $ \textbf{x}^T A \textbf{x} $ = 0.
		\noindent \textbf{(b)}
		Let $ \textbf{x}\in \mathbb{R}^n $ such that $ \left(I_n+A\right)\textbf{x}=\textbf{0} $. Then $ \textbf{x}^T\left(I_n+A\right)\textbf{x} = \textbf{x}^T I_n \textbf{x} + \textbf{x}^T A \textbf{x} = \textbf{x}^T\textbf{x}+0 = \textbf{x}^T \textbf{x} = 0$. Since $ \textbf{x}^T\textbf{x}=0 $, $ \textbf{x}=\textbf{0} $. \\
		$ \therefore $ $ \text{null}(I_n+A)=\{\textbf{0}\} $. Since $ \text{rank}(I_n+A)=n $, by F.T.I.M, $ A+I_n $ is invertible.
	\end{proof}
\end{enumerate}